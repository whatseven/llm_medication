import json
import sys
import os
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from openai import OpenAI

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))

from src.model.config import MODELS, DEFAULT_MODEL
from src.model.prompt import CRITICAL_DOCTOR_EVALUATION_PROMPT

def load_evaluation_results(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½MEDDGè¯„ä¼°ç»“æœæ•°æ®é›†
    
    Args:
        file_path: è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
    
    Returns:
        è¯„ä¼°ç»“æœæ•°æ®åˆ—è¡¨
    """
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                item = json.loads(line)
                # åªå¤„ç†æˆåŠŸçš„æ¡ˆä¾‹
                if item.get('status') == 'success':
                    data.append(item)
    return data

def extract_evaluation_result(response_text: str) -> int:
    """
    ä»å¤§æ¨¡å‹å“åº”ä¸­æå–è¯„ä¼°ç»“æœ
    
    Args:
        response_text: å¤§æ¨¡å‹çš„å®Œæ•´å“åº”æ–‡æœ¬
    
    Returns:
        è¯„ä¼°ç»“æœï¼š1è¡¨ç¤ºæ­£ç¡®ï¼Œ0è¡¨ç¤ºé”™è¯¯ï¼Œ-1è¡¨ç¤ºæå–å¤±è´¥
    """
    try:
        # æŸ¥æ‰¾<r>æ ‡ç­¾
        pattern = r'<r>\s*([01])\s*</r>'
        match = re.search(pattern, response_text, re.IGNORECASE)
        
        if match:
            result = int(match.group(1))
            return result
        
        # å¤‡é€‰æå–æ¨¡å¼
        backup_patterns = [
            r'<r>\s*(\d+)\s*</r>',
            r'<r>([01])</r>',            # æ— ç©ºæ ¼ç‰ˆæœ¬
            r'result[ï¼š:]\s*([01])',
            r'è¯„ä¼°ç»“æœ[ï¼š:]\s*([01])',
            r'æœ€ç»ˆç»“æœ[ï¼š:]\s*([01])'
        ]
        
        for pattern in backup_patterns:
            match = re.search(pattern, response_text, re.IGNORECASE)
            if match:
                result = int(match.group(1))
                if result in [0, 1]:
                    return result
        
        return -1  # æå–å¤±è´¥
        
    except Exception as e:
        print(f"æå–è¯„ä¼°ç»“æœæ—¶å‡ºé”™: {str(e)}")
        return -1

def call_llm_evaluation(item: Dict[str, Any], model_name: str = DEFAULT_MODEL) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹è¿›è¡ŒMEDDGè¯Šæ–­è´¨é‡è¯„ä¼°
    
    Args:
        item: åŒ…å«MEDDGè¯Šæ–­ä¿¡æ¯çš„æ•°æ®é¡¹
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
    
    Returns:
        å¤§æ¨¡å‹çš„è¯„ä¼°å“åº”
    """
    try:
        # è·å–æ¨¡å‹é…ç½®
        model_config = MODELS[model_name]
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = OpenAI(
            api_key=model_config["api_key"],
            base_url=model_config["base_url"]
        )
        
        # æ ¼å¼åŒ–æç¤ºè¯ - ä½¿ç”¨åŸæœ‰æç¤ºè¯æ ¼å¼ï¼Œground_truthè®¾ä¸º"æ— æ ‡å‡†ç­”æ¡ˆ"
        prompt = CRITICAL_DOCTOR_EVALUATION_PROMPT.format(
            input_dialog=item['input_dialog'],
            ground_truth_disease="æ— æ ‡å‡†ç­”æ¡ˆ",
            predicted_diseases=item['predicted_diseases']
        )
        
        # è°ƒç”¨å¤§æ¨¡å‹
        response = client.chat.completions.create(
            model=model_config["model_name"],
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        raise Exception(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")

def process_single_evaluation(item: Dict[str, Any], model_name: str = DEFAULT_MODEL) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªMEDDGè¯„ä¼°é¡¹
    
    Args:
        item: åŒ…å«MEDDGè¯Šæ–­ä¿¡æ¯çš„æ•°æ®é¡¹
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
    
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # è°ƒç”¨å¤§æ¨¡å‹è¯„ä¼°
        start_time = time.time()
        llm_response = call_llm_evaluation(item, model_name)
        end_time = time.time()
        
        # æå–è¯„ä¼°ç»“æœ
        evaluation_result = extract_evaluation_result(llm_response)
        
        result = {
            'id': item['id'],
            'model_prediction': item['predicted_diseases'],
            'input_dialog': item['input_dialog'],
            'raw_diagnosis': item['raw_diagnosis'],
            'llm_evaluation_result': evaluation_result,
            'llm_evaluation_reasoning': llm_response,
            'evaluation_time': round(end_time - start_time, 2),
            'status': 'success' if evaluation_result != -1 else 'extract_failed',
            'dialog_lines_count': item.get('dialog_lines_count', 0),
            'line_range': item.get('line_range', '')
        }
        
        status_symbol = "âœ“" if evaluation_result == 1 else ("âœ—" if evaluation_result == 0 else "âš ")
        print(f"{status_symbol} å®ŒæˆMEDDGè¯„ä¼°ID {item['id']}: {evaluation_result}")
        return result
        
    except Exception as e:
        print(f"âœ— ID {item['id']} MEDDGè¯„ä¼°å¤±è´¥: {str(e)}")
        return {
            'id': item['id'],
            'model_prediction': item['predicted_diseases'],
            'input_dialog': item['input_dialog'],
            'raw_diagnosis': item['raw_diagnosis'],
            'llm_evaluation_result': -1,
            'llm_evaluation_reasoning': f"å¤„ç†é”™è¯¯: {str(e)}",
            'evaluation_time': 0,
            'status': 'error',
            'dialog_lines_count': item.get('dialog_lines_count', 0),
            'line_range': item.get('line_range', '')
        }

def evaluate_meddg_diagnosis_quality(input_file: str, output_file: str, max_workers: int = 100, 
                                   limit: int = None, model_name: str = DEFAULT_MODEL):
    """
    è¯„ä¼°MEDDGè¯Šæ–­è´¨é‡
    
    Args:
        input_file: è¾“å…¥çš„MEDDGè¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„è´¨é‡è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„
        max_workers: å¹¶å‘çº¿ç¨‹æ•°
        limit: é™åˆ¶å¤„ç†çš„æ•°æ®æ¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print(f"å¼€å§‹MEDDGè¯Šæ–­è´¨é‡è¯„ä¼°: {input_file}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½MEDDGè¯„ä¼°ç»“æœæ•°æ®...")
    dataset = load_evaluation_results(input_file)
    
    if limit:
        dataset = dataset[:limit]
        print(f"é™åˆ¶å¤„ç†å‰ {limit} æ¡æ•°æ®")
    
    print(f"æ€»å…± {len(dataset)} æ¡æˆåŠŸçš„MEDDGè¯„ä¼°æ•°æ®")
    
    if len(dataset) == 0:
        print("æ²¡æœ‰å¯å¤„ç†çš„MEDDGæ•°æ®")
        return []
    
    # å¹¶å‘å¤„ç†
    print("\nå¼€å§‹å¹¶å‘MEDDGè´¨é‡è¯„ä¼°...")
    start_time = time.time()
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_item = {
            executor.submit(process_single_evaluation, item, model_name): item 
            for item in dataset
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_item):
            result = future.result()
            results.append(result)
    
    # æŒ‰idæ’åºç¡®ä¿é¡ºåºæ­£ç¡®
    results.sort(key=lambda x: x['id'])
    
    end_time = time.time()
    total_time = round(end_time - start_time, 2)
    
    print(f"\nå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time}ç§’")
    print(f"å¹³å‡æ¯æ¡è€—æ—¶: {round(total_time/len(dataset), 2)}ç§’")
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r['status'] == 'success')
    extract_failed_count = sum(1 for r in results if r['status'] == 'extract_failed')
    error_count = sum(1 for r in results if r['status'] == 'error')
    
    print(f"æˆåŠŸè¯„ä¼°: {success_count}")
    print(f"æå–å¤±è´¥: {extract_failed_count}")
    print(f"å¤„ç†é”™è¯¯: {error_count}")
    
    # ç»Ÿè®¡è¯„ä¼°ç»“æœ
    if success_count > 0:
        correct_count = sum(1 for r in results if r['llm_evaluation_result'] == 1)
        incorrect_count = sum(1 for r in results if r['llm_evaluation_result'] == 0)
        accuracy_rate = correct_count / success_count
        
        print(f"\n=== MEDDGè´¨é‡è¯„ä¼°ç»Ÿè®¡ ===")
        print(f"è¯Šæ–­åˆç†: {correct_count}")
        print(f"è¯Šæ–­ä¸åˆç†: {incorrect_count}")
        print(f"åˆç†ç‡: {accuracy_rate:.4f} ({accuracy_rate*100:.2f}%)")
    
    # ä¿å­˜ç»“æœ
    print(f"\nä¿å­˜ç»“æœåˆ°: {output_file}")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print("MEDDGè´¨é‡è¯„ä¼°å®Œæˆ!")
    return results

def analyze_meddg_evaluation_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ†æMEDDGè¯„ä¼°ç»“æœ
    
    Args:
        results: MEDDGè¯„ä¼°ç»“æœåˆ—è¡¨
    
    Returns:
        åˆ†æç»Ÿè®¡ä¿¡æ¯
    """
    total = len(results)
    success_results = [r for r in results if r['status'] == 'success']
    success_count = len(success_results)
    
    if success_count == 0:
        return {
            'total': total,
            'success_count': 0,
            'quality_accuracy': 0.0
        }
    
    # ç»Ÿè®¡è´¨é‡è¯„ä¼°ç»“æœ
    reasonable_diagnoses = sum(1 for r in success_results if r['llm_evaluation_result'] == 1)
    unreasonable_diagnoses = sum(1 for r in success_results if r['llm_evaluation_result'] == 0)
    
    # ç»Ÿè®¡å¯¹è¯é•¿åº¦åˆ†å¸ƒ
    dialog_lengths = [r['dialog_lines_count'] for r in success_results if 'dialog_lines_count' in r]
    avg_dialog_length = sum(dialog_lengths) / len(dialog_lengths) if dialog_lengths else 0
    
    return {
        'total': total,
        'success_count': success_count,
        'reasonable_diagnoses': reasonable_diagnoses,
        'unreasonable_diagnoses': unreasonable_diagnoses,
        'quality_accuracy': round(reasonable_diagnoses / success_count, 4) if success_count > 0 else 0.0,
        'avg_dialog_lines': round(avg_dialog_length, 2),
        'min_dialog_lines': min(dialog_lengths) if dialog_lengths else 0,
        'max_dialog_lines': max(dialog_lengths) if dialog_lengths else 0
    }


if __name__ == "__main__":
    # é…ç½®æ–‡ä»¶è·¯å¾„
    input_file = "/home/ubuntu/ZJQ/llm_medication/llm_medication/src/data/result/final_result/meddg/graph_rag/meddg_evaluation_results_top5.jsonl"
    output_dir = "/home/ubuntu/ZJQ/llm_medication/llm_medication/src/data/result/final_result/meddg/graph_rag"
    output_file = os.path.join(output_dir, "meddg_quality_evaluation_results.jsonl")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è¿è¡Œè´¨é‡è¯„ä¼°
    print("=== MEDDGå¯¹è¯è¯Šæ–­è´¨é‡è¯„ä¼° ===")
    print("é€‰æ‹©è¯„ä¼°æ¨¡å¼:")
    print("1. æµ‹è¯•æ¨¡å¼(å‰5æ¡)")
    print("2. å°æ‰¹é‡(å‰20æ¡)")  
    print("3. ä¸­æ‰¹é‡(å‰50æ¡)")
    print("4. å…¨é‡è¯„ä¼°")
    
    choice = input("è¯·é€‰æ‹©(1/2/3/4): ").strip()
    
    if choice == '1':
        limit = 5
        max_workers = 2
    elif choice == '2':
        limit = 20
        max_workers = 5
    elif choice == '3':
        limit = 50
        max_workers = 100
    elif choice == '4':
        limit = None
        max_workers = 100
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨æµ‹è¯•æ¨¡å¼")
        limit = 5
        max_workers = 2
    
    # é€‰æ‹©æ¨¡å‹
    print(f"\nå¯ç”¨æ¨¡å‹: {list(MODELS.keys())}")
    model_choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL}): ").strip()
    model_name = model_choice if model_choice in MODELS else DEFAULT_MODEL
    
    # æ‰§è¡Œè´¨é‡è¯„ä¼°
    results = evaluate_meddg_diagnosis_quality(input_file, output_file, max_workers, limit, model_name)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœå¯ä¾›åˆ†æ
    if not results or len(results) == 0:
        print("\nâŒ æ²¡æœ‰è¯„ä¼°ç»“æœå¯ä¾›åˆ†æ")
        exit(1)
    
    print(f"\nğŸ“Š è·å¾— {len(results)} æ¡è¯„ä¼°ç»“æœï¼Œå¼€å§‹åˆ†æ...")
    
    # è¯¦ç»†åˆ†æ
    print("\n=== è¯¦ç»†MEDDGè´¨é‡åˆ†æ ===")
    try:
        analysis = analyze_meddg_evaluation_results(results)
        print(f"âœ… åˆ†æå®Œæˆï¼Œè·å¾— {len(analysis)} é¡¹ç»Ÿè®¡æŒ‡æ ‡")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        print(f"ç»“æœç¤ºä¾‹: {results[0] if results else 'No results'}")
        exit(1)
    
    # é¦–å…ˆæ˜¾ç¤ºæ€»ä½“åˆç†ç‡
    total_accuracy = analysis.get('quality_accuracy', 0.0)
    reasonable_count = analysis.get('reasonable_diagnoses', 0)
    total_count = analysis.get('success_count', 0)
    
    print(f"\nğŸ¯ ã€MEDDGè¯Šæ–­æ€»ä½“åˆç†ç‡ã€‘")
    print(f"=" * 40)
    print(f"è¯„ä¼°ä¸ºåˆç†çš„è¯Šæ–­: {reasonable_count}")
    print(f"æˆåŠŸè¯„ä¼°çš„æ€»æ•°æ®: {total_count}")
    print(f"ğŸ“Š æ€»ä½“åˆç†ç‡: {total_accuracy:.4f} ({total_accuracy*100:.2f}%)")
    print(f"=" * 40)
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    for key, value in analysis.items():
        if key not in ['quality_accuracy', 'reasonable_diagnoses', 'success_count']:  # é¿å…é‡å¤æ˜¾ç¤º
            print(f"{key}: {value}")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("\næ³¨æ„ï¼šMEDDGæ•°æ®é›†æ— ground_truthï¼Œæ­¤è¯„ä¼°åŸºäºå¤§æ¨¡å‹åˆ¤æ–­è¯Šæ–­åˆç†æ€§ã€‚")